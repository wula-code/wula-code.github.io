

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Wula`s Story">
  <meta name="keywords" content="">
  
    <meta name="description" content="Scrapy爬虫一、Scrapy简介Scrapy是由Python语言开发的一款快速、高层次的屏幕抓取和web抓取框架，由于抓取web站点并从页面中提取结构化的数据，只需要实现少量的代码，就能够进行快速的爬取 二、运行原理Scrapy主要包含一下组件：  引擎（Scrapy Engine） Item 项目 调度器（Scheduler） 下载器（Downloader） 爬虫（Spiders） 项目管道">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy爬虫">
<meta property="og:url" content="https://blog.wlstory.xyz/2020/06/13/Scrapy%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="乌拉的故事">
<meta property="og:description" content="Scrapy爬虫一、Scrapy简介Scrapy是由Python语言开发的一款快速、高层次的屏幕抓取和web抓取框架，由于抓取web站点并从页面中提取结构化的数据，只需要实现少量的代码，就能够进行快速的爬取 二、运行原理Scrapy主要包含一下组件：  引擎（Scrapy Engine） Item 项目 调度器（Scheduler） 下载器（Downloader） 爬虫（Spiders） 项目管道">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.wlstory.xyz/img/images/scrapy.webp">
<meta property="article:published_time" content="2020-06-12T16:00:00.000Z">
<meta property="article:modified_time" content="2022-06-22T14:04:37.479Z">
<meta property="article:author" content="Wula&#96;s Story">
<meta property="article:tag" content="Spider">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://blog.wlstory.xyz/img/images/scrapy.webp">
  
  
  
  <title>Scrapy爬虫 - 乌拉的故事</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blog.wlstory.xyz","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  

  

  



  
<meta name="generator" content="Hexo 5.4.2">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>乌拉的故事</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Scrapy爬虫"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2020-06-13 00:00" pubdate>
          2020年6月13日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          8.7k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          73 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Scrapy爬虫</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="Scrapy爬虫"><a href="#Scrapy爬虫" class="headerlink" title="Scrapy爬虫"></a>Scrapy爬虫</h1><h3 id="一、Scrapy简介"><a href="#一、Scrapy简介" class="headerlink" title="一、Scrapy简介"></a>一、Scrapy简介</h3><p><code>Scrapy</code>是由<code>Python</code>语言开发的一款快速、高层次的屏幕抓取和<code>web</code>抓取框架，由于抓取<code>web</code>站点并从页面中提取结构化的数据，只需要实现少量的代码，就能够进行快速的爬取</p>
<h3 id="二、运行原理"><a href="#二、运行原理" class="headerlink" title="二、运行原理"></a>二、运行原理</h3><p><code>Scrapy</code>主要包含一下组件：</p>
<ol>
<li>引擎（Scrapy Engine）</li>
<li>Item 项目</li>
<li>调度器（Scheduler）</li>
<li>下载器（Downloader）</li>
<li>爬虫（Spiders）</li>
<li>项目管道文件（Pipeline）</li>
<li>下载中间件（Downloader Middlewares）</li>
<li>爬虫中间件（Spider Middlewares）</li>
<li>调度中间件（Scheduler Middlewares）</li>
</ol>
<h3 id="三、安装和使用"><a href="#三、安装和使用" class="headerlink" title="三、安装和使用"></a>三、安装和使用</h3><h4 id="3-1-安装"><a href="#3-1-安装" class="headerlink" title="3.1  安装"></a>3.1  安装</h4><p>命令行模式直接使用pip命令安装</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip install scrapy<br></code></pre></td></tr></tbody></table></figure>

<p>第二种：首先下载，然后再安装：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">pip download scrapy -d ./<br><span class="hljs-comment"># 通过指定国内镜像源下载 </span><br>pip download  -i https://pypi.tuna.tsinghua.edu.cn/simple scrapy -d ./<br></code></pre></td></tr></tbody></table></figure>

<p>进入下载目录后执行下面命令安装：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip install Scrapy-<span class="hljs-number">1.5</span><span class="hljs-number">.0</span>-py2.py3-none-<span class="hljs-built_in">any</span>.whl<br></code></pre></td></tr></tbody></table></figure>

<h4 id="3-2-使用"><a href="#3-2-使用" class="headerlink" title="3.2 使用"></a>3.2 使用</h4><p>使用大概分为下面四步1 创建一个scrapy项目</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">scrapy startproject mySpider<br></code></pre></td></tr></tbody></table></figure>

<p>2 生成一个爬虫</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">scrapy genspider demo <span class="hljs-string">"demo.cn"</span><br></code></pre></td></tr></tbody></table></figure>

<p>3 提取数据</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">完善spider 使用xpath等<br></code></pre></td></tr></tbody></table></figure>

<p>4 保存数据</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pipeline中保存数据<br></code></pre></td></tr></tbody></table></figure>

<h4 id="3-3-程序运行"><a href="#3-3-程序运行" class="headerlink" title="3.3 程序运行"></a><strong>3.3 程序运行</strong></h4><p>在命令中运行爬虫</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">scrapy crawl qb     <span class="hljs-comment"># qb爬虫的名字</span><br></code></pre></td></tr></tbody></table></figure>

<p>在<code>pycharm</code>中运行爬虫</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> cmdline<br>cmdline.execute(<span class="hljs-string">"scrapy crawl qb"</span>.split())<br></code></pre></td></tr></tbody></table></figure>

<h3 id="四、基本步骤"><a href="#四、基本步骤" class="headerlink" title="四、基本步骤"></a>四、基本步骤</h3><p><code>Scrapy</code> 爬虫框架的具体使用步骤如下：</p>
<blockquote>
<ol>
<li>选择目标网站</li>
<li>定义要抓取的数据（通过Scrapy Items来完成的）</li>
<li>编写提取数据的spider</li>
<li>执行spider，获取数据</li>
<li>数据存储</li>
</ol>
</blockquote>
<h3 id="五、目录文件说明"><a href="#五、目录文件说明" class="headerlink" title="五、目录文件说明"></a>五、目录文件说明</h3><p>当我们创建了一个scrapy项目后,继续创建了一个spider,目录结构是这样的：</p>
<p>下面来简单介绍一下各个主要文件的作用：</p>
<blockquote>
<p><code>scrapy.cfg</code> ：项目的配置文件</p>
<p><code>mySpider/</code> ：项目的Python模块，将会从这里引用代码</p>
<p><code>mySpider/items.py</code> ：项目的目标文件</p>
<p><code>mySpider/pipelines.py</code> ：项目的管道文件</p>
<p><code>mySpider/settings.py </code>：项目的设置文件</p>
<p><code>mySpider/spiders/ </code>：存储爬虫代码目录</p>
</blockquote>
<h4 id="5-1-scrapy-cfg文件"><a href="#5-1-scrapy-cfg文件" class="headerlink" title="5.1 scrapy.cfg文件"></a>5.1 scrapy.cfg文件</h4><p>项目配置文件。这个是文件的内容：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Automatically created by: scrapy startproject</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># For more information about the [deploy] section see:</span><br><span class="hljs-comment"># https://scrapyd.readthedocs.io/en/latest/deploy.html</span><br><br>[settings]<br>default = mySpider.settings<br><br>[deploy]<br><span class="hljs-comment">#url = http://localhost:6800/</span><br>project = mySpider<br></code></pre></td></tr></tbody></table></figure>

<h4 id="5-2-mySpider-x2F"><a href="#5-2-mySpider-x2F" class="headerlink" title="5.2 mySpider  /"></a>5.2 mySpider  /</h4><p>项目的Python模块，将会从这里引用代码</p>
<h4 id="5-3-mySpider-x2F-items-py"><a href="#5-3-mySpider-x2F-items-py" class="headerlink" title="5.3 mySpider/items.py"></a>5.3 mySpider/items.py</h4><p>项目的目标文件</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define here the models for your scraped items</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># See documentation in:</span><br><span class="hljs-comment"># https://docs.scrapy.org/en/latest/topics/items.html</span><br><br><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyspiderItem</span>(scrapy.Item):<br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    <span class="hljs-comment"># name = scrapy.Field()</span><br>    <span class="hljs-keyword">pass</span><br></code></pre></td></tr></tbody></table></figure>

<p>定义scrapy items的模块,示例: <strong>name = scrapy.Field()</strong></p>
<h4 id="5-4-mySpider-x2F-pipelines-py"><a href="#5-4-mySpider-x2F-pipelines-py" class="headerlink" title="5.4 mySpider/pipelines.py"></a>5.4 mySpider/pipelines.py</h4><p>项目的管道文件</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define your item pipelines here</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span><br><span class="hljs-comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br><br><span class="hljs-comment"># useful for handling different item types with a single interface</span><br><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyspiderPipeline</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        <span class="hljs-keyword">return</span> item<br></code></pre></td></tr></tbody></table></figure>

<p>这个文件也就是我们说的管道,当<code>Item</code>在<code>Spider</code>中被收集之后，它将会被传递到<code>Item Pipeline</code>(管道)，这些<code>Item Pipeline</code>组件按定义的顺序处理<code>Item</code>。每个<code>Item Pipeline</code>都是实现了简单方法的<code>Python</code>类，比如决定此<code>Item</code>是丢弃而存储。以下是<code>item pipeline</code>的一些典型应用：</p>
<ul>
<li>验证爬取的数据(检查<code>item</code>包含某些字段，比如说<code>name</code>字段)</li>
<li>查重(并丢弃)</li>
<li>将爬取结果保存到文件或者数据库中</li>
</ul>
<h4 id="5-5-mySpider-x2F-settings-py"><a href="#5-5-mySpider-x2F-settings-py" class="headerlink" title="5.5 mySpider/settings.py"></a>5.5 mySpider/settings.py</h4><p>项目的设置文件</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Scrapy settings for mySpider project</span><br>...<br><br>BOT_NAME = <span class="hljs-string">'mySpider'</span> <span class="hljs-comment"># scrapy项目名</span><br><br>SPIDER_MODULES = [<span class="hljs-string">'mySpider.spiders'</span>]<br>NEWSPIDER_MODULE = <span class="hljs-string">'mySpider.spiders'</span><br>.......<br><br><span class="hljs-comment"># Obey robots.txt rules</span><br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span> <span class="hljs-comment"># 是否遵守协议,一般给位false,但是创建完项目是是True,我们把它改为False</span><br><br><span class="hljs-comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span><br><span class="hljs-comment">#CONCURRENT_REQUESTS = 32 # 最大并发量 默认16</span><br>......<br><span class="hljs-comment">#DOWNLOAD_DELAY = 3 # 下载延迟 3秒</span><br><br><span class="hljs-comment"># Override the default request headers: # 请求报头,我们打开</span><br>DEFAULT_REQUEST_HEADERS = {<br>  <span class="hljs-string">'Accept'</span>: <span class="hljs-string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,<br>  <span class="hljs-string">'Accept-Language'</span>: <span class="hljs-string">'en'</span>,<br>}<br><span class="hljs-comment"># 爬虫中间件</span><br><span class="hljs-comment">#SPIDER_MIDDLEWARES = {</span><br><span class="hljs-comment">#    'mySpider.middlewares.MyspiderSpiderMiddleware': 543,</span><br><span class="hljs-comment">#}</span><br><br><span class="hljs-comment"># 下载中间件</span><br><span class="hljs-comment">#DOWNLOADER_MIDDLEWARES = {</span><br><span class="hljs-comment">#    'mySpider.middlewares.MyspiderDownloaderMiddleware': 543,</span><br><span class="hljs-comment">#}</span><br>......<br><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><span class="hljs-comment">#ITEM_PIPELINES = {</span><br><span class="hljs-comment">#    'mySpider.pipelines.MyspiderPipeline': 300, # 管道</span><br><span class="hljs-comment">#}</span><br>.......<br></code></pre></td></tr></tbody></table></figure>

<p>省略号省略代码,一般重要点,给了注释</p>
<h4 id="5-6-mySpider-x2F-spiders-x2F-：存储爬虫代码目录"><a href="#5-6-mySpider-x2F-spiders-x2F-：存储爬虫代码目录" class="headerlink" title="5.6.mySpider/spiders/ ：存储爬虫代码目录"></a>5.<strong>6.mySpider/spiders/ ：存储爬虫代码目录</strong></h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DbSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">'db'</span><br>    allowed_domains = [<span class="hljs-string">'douban.com'</span>] <span class="hljs-comment"># 可以修改</span><br>    start_urls = [<span class="hljs-string">'http://douban.com/'</span>] <span class="hljs-comment"># 开始的url也可以修改</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-comment"># pass</span><br></code></pre></td></tr></tbody></table></figure>

<h3 id="六-Scrapy-shell"><a href="#六-Scrapy-shell" class="headerlink" title="六. Scrapy shell"></a><strong>六. Scrapy shell</strong></h3><p><code>Scrapy</code>终端是一个交互终端，我们可以在未启动<code>spider</code>的情况下尝试及调试代码，也可以用来测试<code>XPath</code>或<code>CSS</code>表达式，查看他们的工作方式，方便我们爬取的网页中提取的数据,但是一般使用的不多。感兴趣的查看官方文档:</p>
<p>官方文档</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/shell.html<br></code></pre></td></tr></tbody></table></figure>

<p><code>Scrapy Shell</code>根据下载的页面会自动创建一些方便使用的对象，例如 <code>Response </code>对象，以及 <code>Selector 对象 (对HTML及XML内容)</code>。</p>
<ul>
<li>当<code>shell</code>载入后，将得到一个包含<code>response</code>数据的本地 <code>response </code>变量，输入 <code>response.body</code>将输出<code>response</code>的包体，输出 <code>response.headers</code> 可以看到<code>response</code>的包头。</li>
<li>输入 <code>response.selector</code> 时， 将获取到一个<code>response </code>初始化的类 <code>Selector </code>的对象，此时可以通过使用 <code>response.selector.xpath()</code>或<code>response.selector.css()</code> 来对 <code>response </code>进行查询。</li>
<li>Scrapy也提供了一些快捷方式, 例如 <code>response.xpath()</code>或<code>response.css()</code>同样可以生效（如之前的案例）。</li>
</ul>
<h4 id="Selectors选择器"><a href="#Selectors选择器" class="headerlink" title="Selectors选择器"></a><strong>Selectors选择器</strong></h4><blockquote>
<p>Scrapy Selectors 内置 <strong>XPath</strong> 和 <strong>CSS Selector</strong> 表达式机制</p>
</blockquote>
<p><code>Selector</code>有四个基本的方法，最常用的还是<code>xpath</code>:</p>
<ul>
<li><code>xpath():</code> 传入<code>xpath</code>表达式，返回该表达式所对应的所有节点的<code>selector list</code>列表</li>
<li><code>extract()</code>: 序列化该节点为字符串并返回<code>list</code></li>
<li><code>css():</code> 传入<code>CSS</code>表达式，返回该表达式所对应的所有节点的<code>selector list</code>列表，语法同 <code>BeautifulSoup4</code></li>
<li><code>re()</code>: 根据传入的正则表达式对数据进行提取，返回字符串<code>list</code>列表</li>
</ul>
<h3 id="七、案例实战"><a href="#七、案例实战" class="headerlink" title="七、案例实战"></a><strong>七、案例实战</strong></h3><p>本节，我将使用<code>Scrapy</code>爬取站酷数据作为示例</p>
<h4 id="7-1-案例说明"><a href="#7-1-案例说明" class="headerlink" title="7.1 案例说明"></a>7.1 案例说明</h4><p>既然已经初步了解了<code>scrapy</code>的工作流程以及原理,我们来做一个入门的小案例,爬取站酷首页推荐的<code>item</code>信息。如下图所示，一个小方框就是一个<code>item</code>信息。我们要提取每一个<code>item</code>的六个组成部分：</p>
<ol>
<li><code>imgLink</code>(封面图片链接)；</li>
<li><code>title</code>(标题）;</li>
<li><code>types</code>（类型）;</li>
<li><code>vistor</code>（人气）;</li>
<li><code>comment</code>（评论数）;</li>
<li><code>likes</code>（推荐人数）</li>
</ol>
<p>然后只是一个页面的<code>item</code>，我们还要通过翻页实现批量数据采集。</p>
<h4 id="7-2-文件配置"><a href="#7-2-文件配置" class="headerlink" title="7.2 文件配置"></a>7.2 文件配置</h4><h5 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a><strong>目录结构</strong></h5><p>在上一篇中我们说明了新建**<code>scrapy</code>项目（<code>zcool</code>）*<em>和*</em><code>spider</code>项目（<code>zc</code>）**，这里不再赘述，然后得到我们的目录结构如下图所示：</p>
<h5 id="start-py文件"><a href="#start-py文件" class="headerlink" title="start.py文件"></a><strong>start.py文件</strong></h5><p>然后为了方便运行，在<code>zcool</code>目录下新建<code>start</code>文件。并进行初始化设置。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scrapy <span class="hljs-keyword">import</span> cmdline<br>cmdline.execute(<span class="hljs-string">'scrapy crawl zc'</span>.split())<br></code></pre></td></tr></tbody></table></figure>

<h5 id="settings-py文件"><a href="#settings-py文件" class="headerlink" title="settings.py文件"></a><strong>settings.py文件</strong></h5><p>在这个文件里我们需要做几样设置</p>
<p>避免在程序运行的时候打印<code>log</code>日志信息</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"> LOG_LEVEL = <span class="hljs-string">'WARNING'</span> <br>ROBOTSTXT_OBEY = <span class="hljs-literal">False</span> <br></code></pre></td></tr></tbody></table></figure>

<p>添加请求头：</p>
<p>打开管道：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Configure item pipelines</span><br><span class="hljs-comment"># See https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span><br><br>ITEM_PIPELINEES = {<br>    <span class="hljs-string">'zcool.pipelines.ZcooLPopeline'</span>:<span class="hljs-number">300</span>,<br>}<br></code></pre></td></tr></tbody></table></figure>

<h5 id="item-py文件"><a href="#item-py文件" class="headerlink" title="item.py文件"></a><strong>item.py文件</strong></h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> scrapy<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ZcoolItem</span>(scrapy.Item):<br>    <span class="hljs-comment"># define the fields for your item here like:</span><br>    imgLink = scrapy.Field() <span class="hljs-comment"># 封面图片链接</span><br>    title = scrapy.Field() <span class="hljs-comment"># 标题</span><br>    types = scrapy.Field() <span class="hljs-comment"># 类型</span><br>    vistor = scrapy.Field() <span class="hljs-comment"># 人气</span><br>    comment = scrapy.Field() <span class="hljs-comment"># 评论数</span><br>    likes = scrapy.Field() <span class="hljs-comment"># 推荐人数</span><br></code></pre></td></tr></tbody></table></figure>

<h4 id="7-3-页面数据提取"><a href="#7-3-页面数据提取" class="headerlink" title="7.3 页面数据提取"></a><strong>7.3 页面数据提取</strong></h4><p>首先我们在站酷页面使用<code>xpath-helper</code>测试一下：</p>
<p>然后<code>zc.py</code>文件里面初步测试一下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>    divList = response.xpath(<span class="hljs-string">'//div[@class="work-list-box"]/div'</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(divList))<br></code></pre></td></tr></tbody></table></figure>

<p>没有问题，然后我们对各种信息分别解析提取，</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>    divList = response.xpath(<span class="hljs-string">'//div[@class="work-list-box"]/div'</span>)<br>    <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> divList:<br>        imgLink = div.xpath(<span class="hljs-string">"./div[1]/a/img/@src"</span>).extract()[<span class="hljs-number">0</span>] <span class="hljs-comment"># 1.封面图片链接</span><br>  ...  <span class="hljs-number">2.</span>title(标题）;<span class="hljs-number">3</span> types（类型）;4vistor（人气）;5comment（评论数）  ....<br>        likes = div.xpath(<span class="hljs-string">"./div[2]/p[3]/span[3]/@title"</span>).extract_first() <span class="hljs-comment"># 6likes（推荐人数）</span><br><br>        item = ZcoolItem(imgLink=imgLink,title=title,types=types,vistor=vistor,comment=comment,likes=likes)<br><br>        <span class="hljs-keyword">yield</span> item<br></code></pre></td></tr></tbody></table></figure>

<p><strong>解释：</strong> <strong><code>xpath</code>提取数据方法：</strong></p>
<table>
<thead>
<tr>
<th align="center">S.N.</th>
<th align="center">方法 &amp; 描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>extract()</code></td>
<td align="center">返回的是符合要求的所有的数据，存在一个列表里。</td>
</tr>
<tr>
<td align="center"><code>extract_first()</code></td>
<td align="center">返回的<code>hrefs </code>列表里的第一个数据。</td>
</tr>
<tr>
<td align="center"><code>get()</code></td>
<td align="center">和<code>extract_first()</code>方法返回的是一样的，都是列表里的第一个数据。</td>
</tr>
<tr>
<td align="center"><code>getall()</code></td>
<td align="center">和<code>extract()</code>方法一样，返回的都是符合要求的所有的数据，存在一个列表里。</td>
</tr>
</tbody></table>
<p>注意：</p>
<blockquote>
<p><code>get() </code>、<code>getall() </code>方法是新的方法，<code>extract()</code> 、<code>extract_first()</code>方法是旧的方法。<code>extract()</code> 、<code>extract_first()</code>方法取不到就返回<code>None</code>。<code>get()</code> 、<code>getall() </code>方法取不到就<code>raise</code>一个错误。</p>
</blockquote>
<p><strong>item实例创建（yield上面一行代码）</strong></p>
<p>这里我们之前在目录文件配置的<code>item</code>文件中已经进行了设置，对于数据存储，我们在爬虫文件中开头要导入这个类：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> zcool.items <span class="hljs-keyword">import</span> ZcoolItem<br></code></pre></td></tr></tbody></table></figure>

<p>然后使用<code>yield</code>返回数据。</p>
<p><strong>为什么使用yield而不是return</strong></p>
<p>不能使用<code>return</code>这个无容置疑，因为要翻页，使用return<code>直接</code>退出函数；而对于<code>yield</code>:在调用<code>for</code>的时候，函数内部不会立即执行，只是返回了一个生成器对象。在迭代的时候函数会开始执行，当在<code>yield</code>的时候，会返回当前值(i)。之后的这个函数会在循环中进行，直到没有下一个值。</p>
<h4 id="7-4-翻页实现批量数据采集"><a href="#7-4-翻页实现批量数据采集" class="headerlink" title="7.4 翻页实现批量数据采集"></a><strong>7.4 翻页实现批量数据采集</strong></h4><p>通过上面的代码已经可以初步实现数据采集，只不过只有第一页的，如下图所示：</p>
<p>但是我们的目标是100个页面的批量数据采集，所以代码还需要修改。针对翻页这里介绍两种方式：</p>
<p><strong>方式一</strong>：我们首先在页面中定位到下一页的按钮，如下图所示：</p>
<p>然后编写如下代码，在<code>for</code>循环完毕后。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">next_href = response.xpath(<span class="hljs-string">"//a[@class='laypage_next']/@href"</span>).extract_first()<br><span class="hljs-keyword">if</span> next_href:<br>    next_url = response.urljoin(next_href)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'*'</span> * <span class="hljs-number">60</span>)<br>    <span class="hljs-built_in">print</span>(next_url)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'*'</span> * <span class="hljs-number">60</span>)<br>    request = scrapy.Request(next_url)<br>    <span class="hljs-keyword">yield</span> request<br></code></pre></td></tr></tbody></table></figure>

<p><code>scrapy.Request(): </code>把下一页的<code>url</code>传递给<code>Request</code>函数,进行翻页循环数据采集。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">https://www.cnblogs.com/heymonkey/p/<span class="hljs-number">11818495.</span>html <span class="hljs-comment"># scrapy.Request()参考链接</span><br></code></pre></td></tr></tbody></table></figure>

<p>注意方式一只有下一页按钮它的<strong>href</strong>对应属性值和下一页的url一致才行。</p>
<p><strong>方式二</strong>：定义一个全局变量count = 0,每爬取一页数据，令其加一，构建新的url,再使用scrapy.Request() 发起请求。</p>
<p>如下图所示：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">count = <span class="hljs-number">1</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ZcSpider</span>(scrapy.Spider):<br>    name = <span class="hljs-string">'zc'</span><br>    allowed_domains = [<span class="hljs-string">'zcool.com.cn'</span>]<br>    start_urls = [<span class="hljs-string">'https://www.zcool.com.cn/home?p=1#tab_anchor'</span>] <span class="hljs-comment"># 第一页的url</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">parse</span>(<span class="hljs-params">self, response</span>):<br>        <span class="hljs-keyword">global</span> count<br>        count += <span class="hljs-number">1</span><br>        <br>        <span class="hljs-keyword">for</span> div <span class="hljs-keyword">in</span> divList:<br>    <span class="hljs-comment"># ...xxx...</span><br>            <span class="hljs-keyword">yield</span> item<br><br>        next_url = <span class="hljs-string">'https://www.kuaikanmanhua.com/tag/0?state=1&amp;sort=1&amp;page={}'</span>.<span class="hljs-built_in">format</span>(count)<br>        <span class="hljs-keyword">yield</span> scrapy.Request(next_url)<br></code></pre></td></tr></tbody></table></figure>

<p>这两种方式在实际案例中择机采用。</p>
<h4 id="7-5-数据存储"><a href="#7-5-数据存储" class="headerlink" title="7.5 数据存储"></a><strong>7.5 数据存储</strong></h4><p>数据存储是在<code>pipline.py</code>中进行的,代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> itemadapter <span class="hljs-keyword">import</span> ItemAdapter<br><span class="hljs-keyword">import</span> csv<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ZcoolPipeline</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>): <br>        self.f = <span class="hljs-built_in">open</span>(<span class="hljs-string">'Zcool.csv'</span>,<span class="hljs-string">'w'</span>,encoding=<span class="hljs-string">'utf-8'</span>,newline=<span class="hljs-string">''</span>)       <span class="hljs-comment"># line1</span><br>        self.file_name = [<span class="hljs-string">'imgLink'</span>, <span class="hljs-string">'title'</span>,<span class="hljs-string">'types'</span>,<span class="hljs-string">'vistor'</span>,<span class="hljs-string">'comment'</span>,<span class="hljs-string">'likes'</span>]  <span class="hljs-comment"># line2</span><br>        self.writer = csv.DictWriter(self.f, fieldnames=self.file_name)     <span class="hljs-comment"># line3</span><br>        self.writer.writeheader()              <span class="hljs-comment"># line4</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">process_item</span>(<span class="hljs-params">self, item, spider</span>):<br>        self.writer.writerow(<span class="hljs-built_in">dict</span>(item))              <span class="hljs-comment"># line5</span><br>        <span class="hljs-built_in">print</span>(item)<br>        <span class="hljs-keyword">return</span> item                  <span class="hljs-comment"># line6 </span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">close_spider</span>(<span class="hljs-params">self,spider</span>):<br>        self.f.close()<br></code></pre></td></tr></tbody></table></figure>

<p>解释:</p>
<ul>
<li><code>line1</code>: 打开文件，指定方式为写，利用第3个参数把<code>csv</code>写数据时产生的空行消除</li>
<li><code>line2</code>: 设置文件第一行的字段名，注意要跟<code>spider</code>传过来的字典<code>key</code>名称相同</li>
<li><code>line3</code>: 指定文件的写入方式为<code>csv</code>字典写入，参数1为指定具体文件，参数2为指定字段名</li>
<li><code>line4</code>: 写入第一行字段名，因为只要写入一次，所以文件放在<code>__init__</code>里面</li>
<li><code>line5</code>: 写入<code>spider</code>传过来的具体数值,注意在<code>spider</code>文件中<code>yield</code>的<code>item</code>,是一个由类创建的实例对象，我们写入数据时，写入的是 字典，所以这里还要转化一下。</li>
<li><code>line6</code>: 写入完返回</li>
</ul>
<h4 id="7-6-程序运行"><a href="#7-6-程序运行" class="headerlink" title="7.6 程序运行"></a><strong>7.6 程序运行</strong></h4><p>因为之前创建了<code>start.py</code>文件,并且对它就行了初始化设置，现在运行爬虫程序不需要在控制台中输入命令：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">scrapy crawl zc(爬虫项目名)<br></code></pre></td></tr></tbody></table></figure>

<p>直接运行<code>start.py</code>文件</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E7%88%AC%E8%99%AB/" class="category-chain-item">爬虫</a>
  
  
    <span>></span>
    
  <a href="/categories/%E7%88%AC%E8%99%AB/Scapry/" class="category-chain-item">Scapry</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Spider/">#Spider</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Scrapy爬虫</div>
      <div>https://blog.wlstory.xyz/2020/06/13/Scrapy爬虫/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Wula`s Story</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2020年6月13日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2020/10/20/Django%E6%A1%86%E6%9E%B6/" title="Django基本使用">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Django基本使用</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/06/13/jieba%E5%9F%BA%E7%A1%80/" title="Jieba的学习和使用">
                        <span class="hidden-mobile">Jieba的学习和使用</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> <p>☑️请你用绝对清醒的理智克服不该有的情绪</p> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  
</div>

  
  
    <!-- 备案信息 ICP for China -->
    <div class="beian">
  <span>
    <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
      京ICP证123456号
    </a>
  </span>
  
    
      <span>
        <a
          href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=12345678"
          rel="nofollow noopener"
          class="beian-police"
          target="_blank"
        >
          
            <span style="visibility: hidden; width: 0">|</span>
            <img src="/img/police_beian.png" srcset="/img/loading.gif" lazyload alt="police-icon"/>
          
          <span>京公网安备12345678号</span>
        </a>
      </span>
    
  
</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="/js/leancloud.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"react":{"opacity":0.7}});</script></body>
</html>
